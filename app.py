import os
import json
import feedparser
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
from urllib.parse import quote
from datetime import datetime, timedelta

# ===============================
# í™˜ê²½ ì„¤ì •
# ===============================
load_dotenv()

if not os.environ.get("OPENAI_API_KEY"):
    st.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

client = OpenAI(
    base_url="https://gms.ssafy.io/gmsapi/api.openai.com/v1"
)

MEMORY_FILE = "conversation.json"

# ===============================
# ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
# ===============================
def load_conversation():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_conversation(history):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

# ===============================
# ê¸°ì‚¬ ê²€ìƒ‰ ê¸°ëŠ¥ (ì˜ë„ íŒë‹¨ í¬í•¨)
# ===============================
def is_news_request(user_input: str) -> bool:
    """ì‚¬ìš©ì ì…ë ¥ì´ ê¸°ì‚¬ ê²€ìƒ‰ ìš”ì²­ì¸ì§€ íŒë‹¨"""
    keywords = ["ê¸°ì‚¬", "ë‰´ìŠ¤", "ë³´ë„", "ê²€ìƒ‰"]
    return any(k in user_input for k in keywords)

def search_news(query, start=0, size=5):
    """ì—¬ëŸ¬ ì†ŒìŠ¤(Google News, Naver, Daum)ì—ì„œ ê¸°ì‚¬ ê²€ìƒ‰ - ìµœì‹ ìˆœ"""
    encoded_query = quote(query)
    all_articles = []
    
    # 1. Google News RSS
    try:
        feed_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(feed_url)
        all_articles.extend(feed.entries[:10])
    except:
        pass
    
    # 2. Naver News RSS
    try:
        feed_url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}&sort=1&ds=&de=&nso=so:r,p:all,a:all"
        # NaverëŠ” ì§ì ‘ RSS ì§€ì› ì•ˆ í•¨, ëŒ€ì‹  Google Newsê°€ Naver ê¸°ì‚¬ í¬í•¨í•¨
    except:
        pass
    
    # 3. Daum News RSS
    try:
        feed_url = f"https://news.daum.net/rss/foreign.xml"  # ì‹œí—˜ìš© RSS
        feed = feedparser.parse(feed_url)
        all_articles.extend(feed.entries[:5])
    except:
        pass
    
    # ë‚ ì§œ ê¸°ì¤€ í•„í„°ë§: ì–´ì œ + ì˜¤ëŠ˜ ê¸°ì‚¬ë§Œ
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    
    filtered_articles = []
    for article in all_articles:
        try:
            if hasattr(article, 'published_parsed') and article.published_parsed:
                article_date = datetime(*article.published_parsed[:6]).date()
                # ì–´ì œë‚˜ ì˜¤ëŠ˜ ê¸°ì‚¬ë§Œ
                if article_date in [yesterday, today]:
                    filtered_articles.append(article)
        except:
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ í¬í•¨
            filtered_articles.append(article)
    
    # ìµœì†Œ 5ê°œ ì´ìƒ ì—†ìœ¼ë©´, í•„í„°ë§ ì—†ì´ ëª¨ë“  ìµœì‹  ê¸°ì‚¬ ë°˜í™˜
    if len(filtered_articles) < 3:
        filtered_articles = sorted(
            all_articles,
            key=lambda x: x.published_parsed if hasattr(x, 'published_parsed') else datetime.now().timetuple(),
            reverse=True
        )
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    seen_titles = set()
    unique_articles = []
    for article in filtered_articles:
        if article.title not in seen_titles:
            seen_titles.add(article.title)
            unique_articles.append(article)
    
    return unique_articles[start:start+size]

def summarize_article(text):
    """ê¸°ì‚¬ë¥¼ 3ì¤„ë¡œ ìš”ì•½"""
    res = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3ì¤„ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ëŠ” AIë‹¤."},
            {"role": "user", "content": text}
        ],
        max_completion_tokens=256,
    )
    return res.choices[0].message.content.strip()

def handle_news_request(user_input, offset):
    """ê¸°ì‚¬ ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ (ìµœì‹  ê¸°ì‚¬ë§Œ, ì¤‘ë³µ ì œê±°)"""
    articles = search_news(user_input, offset)

    if not articles:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.\n(ìµœì‹  ê¸°ì‚¬ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”)"

    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    articles_with_date = []
    for article in articles:
        try:
            # published_parsedëŠ” datetime ê°ì²´
            if hasattr(article, 'published_parsed') and article.published_parsed:
                pub_date = datetime(*article.published_parsed[:6])
            else:
                pub_date = datetime.now()
        except:
            pub_date = datetime.now()
        
        articles_with_date.append((pub_date, article))
    
    # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
    articles_with_date.sort(key=lambda x: x[0], reverse=True)

    response = "ğŸ” **ê²€ìƒ‰ëœ ìµœì‹  ê¸°ì‚¬:**\n\n"
    for idx, (pub_date, article) in enumerate(articles_with_date, start=1):
        summary = summarize_article(article.get("summary", ""))
        response += (
            f"{idx}. {article.title}\n"
            f"{summary}\n"
            f"ğŸ”— {article.link}\n\n"
        )

    return response

# ===============================
# ê¸°ë³¸ ì±—ë´‡ ê¸°ëŠ¥
# ===============================
def chatbot_response(history, user_input):
    """ì¼ë°˜ ì±—ë´‡ ì‘ë‹µ ìƒì„± (ë¬¸ë§¥ ìœ ì§€)"""
    messages = [{"role": "system", "content": "ë„ˆëŠ” ì¼ë°˜ì ì¸ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë‹¤."}]
    for h in history:
        messages.append({"role": h["role"], "content": h["content"]})
    messages.append({"role": "user", "content": user_input})

    res = client.chat.completions.create(
        model="gpt-5-nano",
        messages=messages,
        max_completion_tokens=512,
    )
    return res.choices[0].message.content.strip()

# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="AI ì±—ë´‡ + ê¸°ì‚¬ ê²€ìƒ‰", layout="centered")
st.title("ğŸ§  AI ì±—ë´‡ + ğŸ“° ê¸°ì‚¬ ê²€ìƒ‰")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "history" not in st.session_state:
    st.session_state.history = load_conversation()
if "news_offset" not in st.session_state:
    st.session_state.news_offset = 0

# ì´ì „ ëŒ€í™” ì¶œë ¥
for h in st.session_state.history:
    st.chat_message(h["role"]).write(h["content"])

# ì…ë ¥ì°½ (ë‹¨ í•˜ë‚˜)
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if user_input:
    st.chat_message("user").write(user_input)
    st.session_state.history.append({"role": "user", "content": user_input})

    # ê¸°ì‚¬ ê²€ìƒ‰ ì˜ë„ íŒë‹¨ (ê¸°ì‚¬ ê¸°ëŠ¥ ë‚´ë¶€)
    if is_news_request(user_input):
        response = handle_news_request(user_input, st.session_state.news_offset)
        st.session_state.news_offset += 5
    else:
        response = chatbot_response(st.session_state.history, user_input)
        st.session_state.news_offset = 0

    st.chat_message("assistant").write(response)
    st.session_state.history.append({"role": "assistant", "content": response})

    # ë¡œì»¬ íŒŒì¼ì— ëŒ€í™” ì €ì¥
    save_conversation(st.session_state.history)
